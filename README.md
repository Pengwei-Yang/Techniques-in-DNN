# Techniques-in-DNN
This is a demo project for implementing common deep learning techniques.

## Getting Started
In this project, we manually constructed neural networks and implemented common neural network techniques such as Dropout, Weight decay, Batch normalization, and Weight initialization. They engaged in manually constructing neural networks with different architectures and optimized them through parameter tuning. Specifically, the optimization and comparison involved constructing networks based on different optimizers (SGD with Momentum, Adam), various activation functions (ReLU, Leaky ReLU, GeLU), and varying hidden layers and units.

## Example

A training example is shown below as an illustration.
![alt text](https://github.com/Pengwei-Yang/Techniques-in-DNN/blob/main/image.png)


### Prerequisites
Python 3.x version

Check required-libraries.txt in the code folder for all the packages that need to be installed.

Please open an issue if there is any problem.

**Clone the Project:**
```
git clone https://github.com/Pengwei-Yang/Techniques-in-DNN.git
```
### Instructions

**Dataset**

* You can visit the relevant dataset on the shared Google folder:

https://drive.google.com/drive/folders/19fFLlOSbM8MxRT1guYArREwyQ3L2QM5j?usp=sharing

**Code**

* Open and run the Jupyter Notebook in the code folder, and reset the path to your own.

**Paper**

* If you need in-depth instruction on theories, please step into the paper folder.

### Authors
* **Pengwei Yang** -*Main contributor*-
（https://wwww.pengweiyang.com）

* **Chongyangzi Teng** -*Minor contributor*- （https://www.researchgate.net/profile/Chongyangzi-Teng）
* **Mengshen Guo** -*Minor contributor*- （https://www.researchgate.net/profile/Mengshen-Guo-3）


